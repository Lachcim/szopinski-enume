\documentclass{article}

\usepackage{amsmath}
\setlength{\parskip}{1em}

\begin{document}

	\title{ENUME project report\\Project A: linear equation systems\\and matrix
	eigenvalues}
	\author{Michał Szopiński\\\\https://github.com/Lachcim/szopinski-enume}
	\date{November 12, 2020}
	\maketitle
	
	\numberwithin{equation}{section}
	
	\setcounter{section}{-1}
	\section{Abstract}
	
	This project explores the numerical methods of solving systems of linear
	equations and finding eigenvalues of matrices. The following concepts are
	discussed within this document:
	
	\begin{itemize}
		\item Computing the machine epsilon of an environment
		\item Solving linear equation systems using Gaussian elimination with
		partial pivoting
		\item Solving linear equation systems using the Jacobi and Gauss-Seidel
		algorithms
	\end{itemize}
	
	\newpage
	
	\section{Task 1: Computing the machine epsilon}
	
	\subsection{Overview}
	
	The goal of this task was to find the machine epsilon of the MATLAB
	environment, i.e. the maximum relative error of a floating-point
	representation of a number.
	
	\subsection{Implementation}
	
	The computation of the machine epsilon is based on the following two
	observations:
	
	\begin{enumerate}
		\item In binary computer environments, the epsilon is a negative power
		of two.
		\item The machine epsilon can also be expressed as:
		\begin{equation}
			eps = \min\{x \in M: fl(1 + x) > 1 \land x > 0\}
		\end{equation}
		i.e. the smallest representable number such that, when added to $1$,
		has a floating point representation greater than $1$.
	\end{enumerate}
	
	An arbitrary starting point, in this case $2^0$, can thus be chosen, and its
	value can then be sequentially halved until the criterion $fl(1 + x) > 1$ is
	no longer met.
	
	\subsection{Program output}
	
	\begin{verbatim}
		Found epsilon is 2.2204e-16
		Epsilon is 2.2204e-16
	\end{verbatim}
	
	\subsection{Observations}
	
	An epsilon of $2.2204 \cdot 10^{-16} = 2^{-52}$ is indicative of a standard
	double-precision IEEE 754 floating point binary representation.
	
	\newpage
	
	\section{Task 2: Solving systems of linear equations using Gaussian
	elimination}
	
	\subsection{Overview}
	
	The task consisted of two sub-tasks, each specifying a different system
	of equations to solve. The systems were to be solved using Gaussian
	elimination with partial pivoting. The size of the system was sequentially
	doubled until the computation time became prohibitive. The error of each
	solution was to be noted.
	
	Additionally, for systems whose number of equations was equal to 10,
	residual correction was to be applied and the solution was to be noted.
	
	The systems were given by:
	
	\noindent
	\begin{align*}
		&A_{ij}^{(a)} =
		\begin{cases} 
			4 & \text{for } i = j \\
			1 & \text{for } i = j \pm 1 \\
			0 & \text{otherwise}
		\end{cases}
		&&
		b_i^{(a)} = 4 + 0.3i
		\\
		&A_{ij}^{(b)} = \frac{6}{7(i + j + 1)}
		&&
		b_i^{(b)} =
		\begin{cases} 
			\frac{1}{3i} & \text{for even } i\\
			0 & \text{for odd } i
		\end{cases}
	\end{align*}
	
	\subsection{Implementation}
	
	The primary medium upon which the Gaussian elimination is performed is an
	$n$ by $(n + 1)$ matrix (where $n$ is the number of equations), which
	simultaneously describes the coefficient matrix $A$ and the constant vector
	$b$. This matrix is aptly named \texttt{eqsys} in the code.
	
	In essence, the Gaussian elimination method for solving a system of linear
	equations consists of two steps:
	
	\begin{enumerate}
		\item \textbf{Gaussian elimination proper}, which transforms the
		matrix $A$ into a triangular matrix; and
		\item \textbf{Back-substitution}, which transforms the triangular
		matrix into an identity matrix and finds the vector $x$.
	\end{enumerate}
	
	Each of these steps is performed in a separate function.
	
	\subsubsection{Gaussian elimination}
	
	The program travels along the diagonal of \texttt{eqsys} and for each cell
	it encounters, it eliminates (reduces to $0$) the coefficients directly
	below it. It does so by multiplying the current row by a constant and
	subtracting it from the rows beneath. This constant, called
	\texttt{reductor} in the code, is given as the ratio of the cell to be
	reduced and the current diagonal cell.
	
	To combat inaccuracy resulting from subsequent division, multiplication and
	subtraction, the reduced cell is explicitly set to $0$ afterwards.
	
	\subsubsection{Partial pivoting}
	
	Gaussian elimination alone is prone to errors when a value on the diagonal
	is equal to $0$ and the reductor cannot be constructed. For this reason,
	partial pivoting is added.
	
	For each cell on the diagonal, the program looks at the cells beneath it and
	finds the one with the greatest absolute value. If it exceeds the absolute
	value of the current cell, the corresponding two rows are swapped. This
	ensures that the divisor making up the reductor is as far from zero as
	possible.
	
	\subsubsection{Back-substitution}
	
	Back-substitution is analogous to Gaussian elimination, except it travels
	leftwards along the diagonal end eliminates the cells above it. In addition,
	it performs ``normalization" of the diagonal, scaling each row so that the
	diagonal cell is equal to $1$.
	
	\subsubsection{Residual correction}
	
	Residual correction is a method of improving the accuracy of the results.
	The system is solved once again, this time with $b$ replaced with the vector
	of errors of the solution. The resulting vector $\delta x$ is then
	subtracted from the original solution. This procedure can be repeated as
	needed.
	
\end{document}
